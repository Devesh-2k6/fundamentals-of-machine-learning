import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_percentage_error
from sklearn import svm
from sklearn.linear_model import LinearRegression

# Load dataset
dataset = pd.read_csv("C:/Users/DEVESH/Documents/Dataset/New folder/HousePricePrediction.csv")

# Display first 5 rows
print(dataset.head(5))
print("Dataset shape:", dataset.shape)

# Identify column types
obj = (dataset.dtypes == 'object')
object_cols = list(obj[obj].index)
print("Categorical variables:", len(object_cols))

int_ = (dataset.dtypes == 'int')
num_cols = list(int_[int_].index)
print("Integer variables:", len(num_cols))

fl = (dataset.dtypes == 'float')
fl_cols = list(fl[fl].index)
print("Float variables:", len(fl_cols))

# Correlation heatmap (only numeric columns)
plt.figure(figsize=(12, 6))
sns.heatmap(dataset.select_dtypes(include=['number']).corr(),
            cmap='BrBG',
            fmt='.2f',
            linewidths=2,
            annot=True)
plt.title('Correlation Heatmap')
plt.show()

# Unique categorical value counts
unique_values = []
for col in object_cols:
    unique_values.append(dataset[col].nunique())

plt.figure(figsize=(10, 6))
plt.title('No. Unique values of Categorical Features')
sns.barplot(x=object_cols, y=unique_values)
plt.xticks(rotation=90)
plt.show()

# Distribution of categorical features
plt.figure(figsize=(18, 3 * len(object_cols)))
index = 1

for col in object_cols:
    y = dataset[col].value_counts()
    plt.subplot(len(object_cols), 1, index)
    sns.barplot(x=list(y.index), y=y)
    plt.title(f'{col} Distribution')
    plt.xticks(rotation=90)
    index += 1

plt.tight_layout()
plt.show()

# Drop ID column and handle missing values
dataset.drop(['Id'], axis=1, inplace=True)
dataset['SalePrice'] = dataset['SalePrice'].fillna(dataset['SalePrice'].mean())
new_dataset = dataset.dropna()
print("Missing values after cleaning:\n", new_dataset.isnull().sum().sum())

# One-Hot Encoding for categorical features
s = (new_dataset.dtypes == 'object')
object_cols = list(s[s].index)
print("Categorical variables:", object_cols)
print('No. of categorical features: ', len(object_cols))

OH_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
OH_cols = pd.DataFrame(OH_encoder.fit_transform(new_dataset[object_cols]))
OH_cols.index = new_dataset.index
OH_cols.columns = OH_encoder.get_feature_names_out(object_cols)

# Combine numerical and encoded categorical data
df_final = new_dataset.drop(object_cols, axis=1)
df_final = pd.concat([df_final, OH_cols], axis=1)

# Split data
X = df_final.drop(['SalePrice'], axis=1)
Y = df_final['SalePrice']
X_train, X_valid, Y_train, Y_valid = train_test_split(X, Y, train_size=0.8, test_size=0.2, random_state=0)

# SVR Model
model_SVR = svm.SVR()
model_SVR.fit(X_train, Y_train)
Y_pred_svr = model_SVR.predict(X_valid)
print("SVR Mean Absolute Percentage Error:", mean_absolute_percentage_error(Y_valid, Y_pred_svr))

# Linear Regression Model
model_LR = LinearRegression()
model_LR.fit(X_train, Y_train)
Y_pred_lr = model_LR.predict(X_valid)
print("Linear Regression Mean Absolute Percentage Error:", mean_absolute_percentage_error(Y_valid, Y_pred_lr))
